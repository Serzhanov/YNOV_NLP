{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 ère partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL to perform standard imports:\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Créer un objet Doc à partir du fichier `owlcreek.txt`**<br>\n",
    "> HINT: Use `with open('../TextFiles/owlcreek.txt') as f:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n",
    "with open('../data/owlcreek.txt') as f:\n",
    "    text=f.read()\n",
    "doc=nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AN OCCURRENCE AT OWL CREEK BRIDGE\n",
       "\n",
       "by Ambrose Bierce\n",
       "\n",
       "I\n",
       "\n",
       "A man stood upon a railroad bridge in northern Alabama, looking down\n",
       "into the swift water twenty feet below.  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to verify it worked:\n",
    "\n",
    "doc[:36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Combien de tokens sont contenus dans le fichier ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4835"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Combien de phrases sont contenues dans le fichier ?<br>HINT: Vous devez d'abord créer une liste !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "sentences=[i for i in doc.sents]\n",
    "print(len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences2=sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Imprimer la deuxième phrase du document**<br> HINT: L'indexation commence à zéro et le titre compte comme la première phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man stood upon a railroad bridge in northern Alabama, looking down\n",
      "into the swift water twenty feet below.  \n"
     ]
    }
   ],
   "source": [
    "first_phrase=str(sentences[0]).split('\\n\\n')[-1]\n",
    "print(first_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Pour chaque élément de la phrase ci-dessus, imprimez son`text`, `POS` tag, `dep` tag and `lemma`<br>\n",
    "CHALLENGE: Faire en sorte que les valeurs s'alignent en colonnes dans la sortie imprimée.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A DET det a\n",
      "man NOUN nsubj man\n",
      "stood VERB ROOT stand\n",
      "upon SCONJ prep upon\n",
      "a DET det a\n",
      "railroad NOUN compound railroad\n",
      "bridge NOUN pobj bridge\n",
      "in ADP prep in\n",
      "northern ADJ amod northern\n",
      "Alabama PROPN pobj Alabama\n",
      ", PUNCT punct ,\n",
      "looking VERB advcl look\n",
      "down ADV advmod down\n",
      "\n",
      " SPACE dep \n",
      "\n",
      "into ADP prep into\n",
      "the DET det the\n",
      "swift ADJ amod swift\n",
      "water NOUN pobj water\n",
      "twenty NUM nummod twenty\n",
      "feet NOUN npadvmod foot\n",
      "below ADV advmod below\n",
      ". PUNCT punct .\n",
      "  SPACE dep  \n"
     ]
    }
   ],
   "source": [
    "# NORMAL SOLUTION:\n",
    "doc2=nlp(first_phrase)\n",
    "for token in doc2:\n",
    "    print(token.text, token.pos_, token.dep_,token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A          DET        det       a\n",
      "man        NOUN       nsubj     man\n",
      "stood      VERB       ROOT      stand\n",
      "upon       SCONJ      prep      upon\n",
      "a          DET        det       a\n",
      "railroad   NOUN       compound  railroad\n",
      "bridge     NOUN       pobj      bridge\n",
      "in         ADP        prep      in\n",
      "northern   ADJ        amod      northern\n",
      "Alabama    PROPN      pobj      Alabama\n",
      ",          PUNCT      punct     ,\n",
      "looking    VERB       advcl     look\n",
      "down       ADV        advmod    down\n",
      "\n",
      "          SPACE      dep       \n",
      "\n",
      "into       ADP        prep      into\n",
      "the        DET        det       the\n",
      "swift      ADJ        amod      swift\n",
      "water      NOUN       pobj      water\n",
      "twenty     NUM        nummod    twenty\n",
      "feet       NOUN       npadvmod  foot\n",
      "below      ADV        advmod    below\n",
      ".          PUNCT      punct     .\n",
      "           SPACE      dep        \n"
     ]
    }
   ],
   "source": [
    "# CHALLENGE SOLUTION:\n",
    "\n",
    "\n",
    "for token in doc2:\n",
    "    print(f'{token.text:<{10}} {token.pos_:<{10}} {token.dep_:<{10}}{token.lemma_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 ème partie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1: Tokenisation de phrases avec SpaCy\n",
    "\n",
    "Question : Créez une phrase complexe en anglais et utilisez SpaCy pour la tokeniser en phrases individuelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Let's say that the scientific community has made significant advancements in understanding climate change., The challenge remains to translate this knowledge into actionable policies that can mitigate its adverse effects, as governments and societies grapple with the urgency of the issue.]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# The complex sentence\n",
    "complex_sentence = \"Let's say that the scientific community has made significant advancements in understanding climate change. The challenge remains to translate this knowledge into actionable policies that can mitigate its adverse effects, as governments and societies grapple with the urgency of the issue.\"\n",
    "\n",
    "# Tokenize the complex sentence into individual sentences\n",
    "doc = nlp(complex_sentence)\n",
    "\n",
    "# Extract individual sentences\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "print(sentences)\n",
    "print(len(sentences))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Utilisez SpaCy pour tokeniser la phrase complexe en mots.\n",
    "\n",
    "Question : Quels avantages offre SpaCy par rapport à d'autres bibliothèques pour la tokenisation?\n",
    "\n",
    "Question : Créez une fonction custom_spacy_tokenizer qui prend une phrase en entrée, utilise SpaCy pour la tokeniser et renvoie les tokens en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction custom_spacy_tokenizer à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "say\n",
      "that\n",
      "the\n",
      "scientific\n",
      "community\n",
      "has\n",
      "made\n",
      "significant\n",
      "advancements\n",
      "in\n",
      "understanding\n",
      "climate\n",
      "change\n",
      ".\n",
      "The\n",
      "challenge\n",
      "remains\n",
      "to\n",
      "translate\n",
      "this\n",
      "knowledge\n",
      "into\n",
      "actionable\n",
      "policies\n",
      "that\n",
      "can\n",
      "mitigate\n",
      "its\n",
      "adverse\n",
      "effects\n",
      ",\n",
      "as\n",
      "governments\n",
      "and\n",
      "societies\n",
      "grapple\n",
      "with\n",
      "the\n",
      "urgency\n",
      "of\n",
      "the\n",
      "issue\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation en mots avec SpaCy\n",
    "for token in doc:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'s\", 'say', 'that', 'the', 'scientific', 'community', 'has', 'made', 'significant', 'advancements', 'in', 'understanding', 'climate', 'change', '.', 'the', 'challenge', 'remains', 'to', 'translate', 'this', 'knowledge', 'into', 'actionable', 'policies', 'that', 'can', 'mitigate', 'its', 'adverse', 'effects', ',', 'as', 'governments', 'and', 'societies', 'grapple', 'with', 'the', 'urgency', 'of', 'the', 'issue', '.']\n"
     ]
    }
   ],
   "source": [
    "# Fonction de tokenisation personnalisée avec SpaCy\n",
    "\n",
    "\n",
    "def custom_tokenizer_stemmer(sentence):\n",
    "    # Tokenisez la phrase avec SpaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    return [token.text.lower() for token in doc]\n",
    "\n",
    "# Exemple d'utilisation\n",
    "tokens = custom_tokenizer_stemmer(complex_sentence)\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 3\n",
    "\n",
    "Question : Utilisez SpaCy pour extraire les lemmes et les POS de la phrase complexe.\n",
    "\n",
    "Question : Comment SpaCy gère-t-il la tokenisation des entités nommées? Testez cela sur une phrase contenant une entité nommée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Apple, Lemme: Apple, POS: PROPN\n",
      "Token: is, Lemme: be, POS: AUX\n",
      "Token: a, Lemme: a, POS: DET\n",
      "Token: major, Lemme: major, POS: ADJ\n",
      "Token: technology, Lemme: technology, POS: NOUN\n",
      "Token: company, Lemme: company, POS: NOUN\n",
      "Token: ., Lemme: ., POS: PUNCT\n",
      "\n",
      "Token: Apple, Entité nommée: ORG\n",
      "Token: is, Entité nommée: \n",
      "Token: a, Entité nommée: \n",
      "Token: major, Entité nommée: \n",
      "Token: technology, Entité nommée: \n",
      "Token: company, Entité nommée: \n",
      "Token: ., Entité nommée: \n"
     ]
    }
   ],
   "source": [
    "# Tokenisation des entités nommées avec SpaCy\n",
    "text_with_entity = \"Apple is a major technology company.\"\n",
    "\n",
    "\n",
    "doc2=nlp(text_with_entity)\n",
    "# Extrayez les lemmes et les POS\n",
    "for token in doc2:\n",
    "    print(f\"Token: {token.text}, Lemme: {token.lemma_}, POS: {token.pos_}\")\n",
    "print()\n",
    "for token in doc2:\n",
    "    print(f\"Token: {token.text}, Entité nommée: {token.ent_type_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execice 1\n",
    "\n",
    "Question : Créez une phrase complexe en anglais.\n",
    "\n",
    "Question : Utilisez SpaCy pour effectuer le stemming sur les mots de la phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots d'origine: ['The', 'scientific', 'community', 'has', 'made', 'significant', 'advancements', 'in', 'understanding', 'climate', 'change', '.', 'The', 'challenge', 'remains', 'to', 'translate', 'this', 'knowledge', 'into', 'actionable', 'policies', 'that', 'can', 'mitigate', 'its', 'adverse', 'effects', ',', 'as', 'governments', 'and', 'societies', 'grapple', 'with', 'the', 'urgency', 'of', 'the', 'issue', '.']\n",
      "Mots stemmés: ['stem', 'is', 'an', 'essenti', 'part', 'of', 'natur', 'languag', 'process', '.', 'it', 'involv', 'reduc', 'word', 'to', 'their', 'base', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Stemming is an essential part of natural language processing. It involves reducing words to their base form.\"\n",
    "doc3=nlp(text)\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_words = [stemmer.stem(token.text) for token in doc3]\n",
    "\n",
    "print(\"Mots d'origine:\", [token.text for token in doc])\n",
    "print(\"Mots stemmés:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Créez une fonction custom_spacy_stemmer qui prend une phrase en entrée, utilise SpaCy pour effectuer le stemming, et renvoie les stems en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction custom_spacy_stemmer à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stemming', 'be', 'an', 'essential', 'part', 'of', 'natural', 'language', 'processing', '.', 'it', 'involve', 'reduce', 'word', 'to', 'their', 'base', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def custom_spacy_stemmer(sentence):\n",
    "    # Tokenisez la phrase avec SpaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Effectuez le stemming en utilisant la racine (lemma) de chaque token\n",
    "    stemmed_words = [token.lemma_.lower() for token in doc]\n",
    "    \n",
    "    return stemmed_words\n",
    "\n",
    "# Exemple d'utilisation\n",
    "stemmed_words = custom_spacy_stemmer(text)\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1 \n",
    "\n",
    "Question : Créez une phrase complexe en anglais.\n",
    "\n",
    "Question : Utilisez SpaCy pour effectuer la lemmatisation sur les mots de la phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots d'origine: ['Lemmatization', 'is', 'an', 'essential', 'part', 'of', 'natural', 'language', 'processing', '.', 'It', 'involves', 'reducing', 'words', 'to', 'their', 'base', 'form', '.']\n",
      "Mots lemmatisés: ['lemmatization', 'be', 'an', 'essential', 'part', 'of', 'natural', 'language', 'processing', '.', 'it', 'involve', 'reduce', 'word', 'to', 'their', 'base', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Lemmatization is an essential part of natural language processing. It involves reducing words to their base form.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Effectuez la lemmatisation sur les tokens\n",
    "lemmatized_words = [token.lemma_ for token in doc]\n",
    "\n",
    "# Affichez les mots lemmatisés\n",
    "print(\"Mots d'origine:\", [token.text for token in doc])\n",
    "print(\"Mots lemmatisés:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Créez une fonction custom_spacy_lemmatizer qui prend une phrase en entrée, utilise SpaCy pour effectuer la lemmatisation, et renvoie les lemmes en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction custom_spacy_lemmatizer à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lemmatization',\n",
       " 'be',\n",
       " 'an',\n",
       " 'essential',\n",
       " 'part',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'it',\n",
       " 'involve',\n",
       " 'reduce',\n",
       " 'word',\n",
       " 'to',\n",
       " 'their',\n",
       " 'base',\n",
       " 'form',\n",
       " '.']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fonction de lemmatisation personnalisée avec SpaCy\n",
    "def custom_spacy_lemmatizer(sentence):\n",
    "    # Tokenisez la phrase avec SpaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Effectuez la lemmatisation en utilisant le lemme de chaque token en minuscules\n",
    "    lemmatized_words = [token.lemma_.lower() for token in doc]\n",
    "    \n",
    "    return lemmatized_words\n",
    "\n",
    "custom_spacy_lemmatizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1\n",
    "\n",
    "Question : Créez une phrase complexe en anglais.\n",
    "\n",
    "Question : Utilisez SpaCy pour filtrer les stopwords de la phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots d'origine: ['Stopwords', 'are', 'common', 'words', 'that', 'are', 'often', 'removed', 'during', 'text', 'preprocessing', '.']\n",
      "Mots non-stopwords: ['Stopwords', 'common', 'words', 'removed', 'text', 'preprocessing', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Téléchargement du modèle de langue anglaise\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Création d'une phrase complexe\n",
    "text = \"Stopwords are common words that are often removed during text preprocessing.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Filtrer les stopwords\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "# Affichez les mots non-stopwords\n",
    "print(\"Mots d'origine:\", [token.text for token in doc])\n",
    "print(\"Mots non-stopwords:\", filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Créez une fonction remove_stopwords qui prend une phrase en entrée, utilise SpaCy pour filtrer les stopwords, et renvoie les mots restants en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction remove_stopwords à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stopwords', 'common', 'words', 'removed', 'text', 'preprocessing', '.']\n"
     ]
    }
   ],
   "source": [
    "# Fonction de filtrage des stopwords avec SpaCy\n",
    "def remove_stopwords(sentence):\n",
    "    # Tokenisez la phrase avec SpaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Filtrer les stopwords et renvoyer les mots restants en minuscules\n",
    "    filtered_words = [token.text.lower() for token in doc if not token.is_stop]\n",
    "    \n",
    "    return filtered_words\n",
    "\n",
    "# Exemple d'utilisation\n",
    "filtered_words = remove_stopwords(text)\n",
    "print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codez une fonction 'preprocess_text_spacy' qui applique les méthodes de preprocessing vues en cours et permet d'obtenir les résultats prétraités suivants :\n",
    "\n",
    "NB : n'oublier pas de traiter les caractères spéciaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_spacy(text):\n",
    "    # Supprimer les caractères spéciaux et convertir en minuscules\n",
    "    text = ''.join(char.lower() for char in text if char.isalnum() or char.isspace())\n",
    "\n",
    "    # Tokenisation avec SpaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Filtrer les stopwords et effectuer la lemmatisation en minuscules\n",
    "    processed_words = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    # Rejoindre les mots en une seule chaîne de texte\n",
    "    processed_text = ' '.join(processed_words)\n",
    "\n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello know natural language processing fascinating field'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple d'utilisation avec SpaCy\n",
    "raw_text = \"Hello, I know that Natural Language Processing is a fascinating field!!!\"\n",
    "\n",
    "preprocess_text_spacy(raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2278239141.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[96], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Texte prétraité avec SpaCy: hello know natural language processing fascinating field\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Texte prétraité avec SpaCy: hello know natural language processing fascinating field"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
